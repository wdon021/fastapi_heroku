from opencensus.ext.azure.trace_exporter import AzureExporter
# https://opencensus.io/exporters/supported-exporters/python/azuremonitor/
# Azure Monitor is an extensible Application Performance Management (APM) service for web developers on multiple platforms.
# Offered by Microsoft Azure, itâ€™s a complete at-scale telemetry and monitoring solution.
from opencensus.trace.samplers import ProbabilitySampler
# You may specify a sampler as part of your Tracer configuration.
#  If no explicit sampler is provided, the ProbabilitySampler will be used by default.
# The ProbabilitySampler would use a rate of 1/10000 by default,
#  meaning one out of every 10000 requests will be sent to Application Insights.
# If you want to specify a sampling rate,
# make sure your Tracer specifies a sampler with a sampling rate between 0.0 and 1.0 inclusive.
from opencensus.trace.tracer import Tracer
# https://docs.microsoft.com/en-us/azure/azure-monitor/app/opencensus-python
# Logs, metrics, and traces are often known as the three pillars of observability.
# In OpenCensus, tracing refers to distributed tracing.
# The AzureExporter sends requests and dependency telemetry to Azure Monitor.
from opencensus.trace.span import SpanKind

from opencensus.trace.attributes_helper import COMMON_ATTRIBUTES

# FastAPI imports
from fastapi import FastAPI, Request, UploadFile, File

import uvicorn
from iris import Iris
import pickle
import pandas as pd
from typing import List
# 2. Create the app object
app = FastAPI()

# 2.1 load Machine learning model
with open('./rf.pkl', 'rb') as model_file:
    classifier = pickle.load(model_file)


# FASTAPI MIDDLEWARE FOR OPENCENSUS
# found in Application Insights, Overview - Instrumentation Key
APPINSIGHTS_INSTRUMENTATIONKEY = '694102c9-3c4a-46f1-acdf-b2efa5732fd8'


HTTP_URL = COMMON_ATTRIBUTES['HTTP_URL']
HTTP_STATUS_CODE = COMMON_ATTRIBUTES['HTTP_STATUS_CODE']

# fastapi middleware for opencensus
# https://fastapi.tiangolo.com/tutorial/middleware/
# Middleware is a functio nthat works with every request before it is processed by any specific path operation.
# and also with every response before returning it.
# 1. it takes each request that comes to your application.
# 2. it  can then do something to that request or run any needed code.
# 3. Then it passes the request to be processed by the rest of the application
# 4. it then takes the response generated by the application
# 5. it can do something to that response or run any needed code
# 6. then it returns the response

@app.middleware("http") # always decorate with .middleware("http")
async def middlewareOpencensus(request: Request, call_next): # The middleware function recieves: The request, A function call_next that will recieve the request as a parameter
    tracer = Tracer(exporter=AzureExporter(connection_string=f'InstrumentationKey={APPINSIGHTS_INSTRUMENTATIONKEY}'),sampler=ProbabilitySampler(1.0))
    # https://opencensus.io/tracing/span/kind/#2
    # SpanKind details the relationships between spans in addition to the parent/child relationship.
    # Spand kind: 0 = UNSEPCIFIED, 1 = SERVER, 2 = CLIENT
    # Detail explaination of Span : https://opencensus.io/tracing/span/
    with tracer.span("main") as span:
        span.span_kind = SpanKind.SERVER

        response = await call_next(request) # call next will pass the request to the corresponding path operation, then returns the response genrated by the corresponding path operation

        tracer.add_attribute_to_current_span( # current span is a SERVER
            attribute_key=HTTP_STATUS_CODE, # E.g. 202, 201, 404, 405, 505
            attribute_value=response.status_code)
        tracer.add_attribute_to_current_span(
            attribute_key=HTTP_URL,
            attribute_value=str(request.url))

    return response

# Default page return Json format string
# get request to get information from web server
@app.get('/')
def index():
    return {"Message":"Welcome"}

# Post Operation to send data to the web server as a request body
# request body is data sent by the client to the API
# then API return a respnse body to the client.

async def model(tags: List[list] = []): # List[float] also works
    prediction = classifier.predict([tags])
    return str(prediction)

@app.post('/predict')
async def predict(data:Iris): # Declare it as a parameter after Iris data model been created
    # convert Iris object into dictionary
    data = data.dict()
    print("in the prediction")
    # There are different way of input data
    # another way is to input as csv file using: fastapi.UploadFile
    sepal_length = data['sepal_length']
    sepal_width = data['sepal_width']
    petal_length = data['petal_length']
    petal_width = data['petal_width']
    prediction = await model([sepal_length, sepal_width, petal_length, petal_width])
    print("prediction is done")
    # can control the return by specify the response_model inside @app.post() decorator
    return {"prediction": str(prediction)}

@app.post("/files")
async def external_file(file: UploadFile = File(...)):

    pd_file = pd.read_csv(file.file, header = None)
    result = classifier.predict(pd_file)
    return {'prediction': str(list(result))}


if __name__ == "__main__":
    # another way to run is to not call uvicorn.run() in side main.py script
    # but call: uvicorn main:app --reload in the bash.
    uvicorn.run("main:app", host="0.0.0.0", port=8080) # 0.0.0.0
